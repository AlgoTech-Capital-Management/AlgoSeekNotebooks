{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Introduction to Using SEC Edgar Data with AlgoSeek Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Prerequisite\n",
    "Before you run through this notebook, go to [SEC Edgar](https://www.sec.gov/edgar/sec-api-documentation), scroll down to the bottom of the page, and click on the [companyfacts.zip](http://www.sec.gov/Archives/edgar/daily-index/xbrl/companyfacts.zip) to download it to your computer. Unzip it into your data folder. Since there are limitations to the Edgar APIs, it's easier to bulk download the raw files and process them locally. The unzipped folder should have a bunch of json files that look like 'CIK0000001750.json'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This notebook demonstrates how to fetch sec edgar filing data and use it with AlgoSeek's Equity Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.1) Example using the Edgar API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In order to use the SEC Edgar API, you must use a valid email address. This is simply an anti-bot measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# headers = {'User-Agent': \"your@email.com\"}\n",
    "headers = {'User-Agent': \"julian@julianwiley.com\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Fetch SEC Edgar index of tickers and CIK numbers for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tickers_cik = requests.get(\"https://www.sec.gov/files/company_tickers.json\", headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tickers_cik.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tickers_cik = pd.json_normalize(pd.json_normalize(tickers_cik.json(),\n",
    "max_level=0).values[0])\n",
    "tickers_cik[\"cik_str\"] = tickers_cik[\"cik_str\"].astype(str).str.zfill(10)\n",
    "tickers_cik.set_index(\"ticker\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tickers_cik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tickers_cik.to_parquet('data/us_equity/reference/cik_ref.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aapl_facts = requests.get(f\"https://data.sec.gov/api/xbrl/companyfacts/CIK0000320193.json\",headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aapl_f = aapl_facts.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(aapl_f['facts']['us-gaap'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in aapl_f['facts']['us-gaap'].keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in aapl_f['facts']['dei'].keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in aapl_f['facts']['dei'].keys():\n",
    "            fact = aapl_f['facts']['dei'][key]\n",
    "\n",
    "            # USD\n",
    "\n",
    "            try:\n",
    "                fact = pd.json_normalize(fact['units']['USD'])\n",
    "                fact[\"filed\"] = pd.to_datetime(fact[\"filed\"])\n",
    "                fact = fact.sort_values(\"end\")\n",
    "                print(fact)\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aapl_f['facts']['us-gaap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.json_normalize(aapl_f['facts']['us-gaap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aapl_assets = aapl_f['facts']['us-gaap']['Assets']\n",
    "aapl_assets = pd.json_normalize(aapl_assets['units']['USD'])\n",
    "aapl_assets[\"filed\"] = pd.to_datetime(aapl_assets[\"filed\"])\n",
    "aapl_assets = aapl_assets.sort_values(\"end\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aapl_assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.2) Using the files from our intial bulk download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ex1 = pd.read_json('data/us_equity/companyfacts/CIK0000320193.json')\n",
    "ex1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in ex1['facts']['us-gaap'].keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in ex1['facts']['dei'].keys():\n",
    "            fact = ex1['facts']['dei'][key]\n",
    "\n",
    "            # USD\n",
    "\n",
    "            try:\n",
    "                fact = pd.json_normalize(fact['units']['USD'])\n",
    "                fact[\"filed\"] = pd.to_datetime(fact[\"filed\"])\n",
    "                fact = fact.sort_values(\"end\")\n",
    "                print(fact)\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tick in tickers_cik['cik_str']:\n",
    "    file_dir = f'data/us_equity/companyfacts/CIK{tick}.json'.format(tick=tick)\n",
    "    print(os.path.isfile(file_dir))\n",
    "    # print(tick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index, row in tickers_cik.iterrows():\n",
    "    file_dir = f'data/us_equity/companyfacts/CIK{tick}.json'.format(tick=index)\n",
    "    print(os.path.isfile(file_dir))\n",
    "    print(row['cik_str'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Fetching Financial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2) Download SEC filings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/julia/Documents/Coding/JupyterSamples/AlgoSeekNotebooks')\n",
    "look = pd.read_parquet('data/us_equity/reference/lookup.parquet')\n",
    "ref = pd.read_parquet('data/us_equity/reference/master.parquet')\n",
    "tickers = look['Ticker'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tickers_cik['cik_str'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# os.mkdir('data/us_equity/fundamental/')\n",
    "# os.mkdir('data/us_equity/fundamental/sec/')\n",
    "for index, row in tickers_cik.iterrows():\n",
    "    print(index)\n",
    "    ticker = index\n",
    "\n",
    "    file_dir = 'data/us_equity/companyfacts/CIK'+row['cik_str']+'.json'\n",
    "\n",
    "    try:\n",
    "        os.mkdir('data/us_equity/fundamentals/sec/{tick}/'.format(tick=ticker))\n",
    "    except:\n",
    "        print('mkdir failed for ticker: '+ticker)\n",
    "        pass\n",
    "\n",
    "    cik = row['cik_str']\n",
    "    # print(ticker, cik)\n",
    "    try:\n",
    "        company_facts = pd.read_json(file_dir)\n",
    "        company_facts.head()\n",
    "    except:\n",
    "        print(ticker + ' error')\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "\n",
    "        for key in company_facts['facts']['dei'].keys():\n",
    "            fact = company_facts['facts']['dei'][key]\n",
    "\n",
    "            # USD\n",
    "\n",
    "            try:\n",
    "                fact = pd.json_normalize(fact['units']['USD'])\n",
    "                fact[\"filed\"] = pd.to_datetime(fact[\"filed\"])\n",
    "                fact = fact.sort_values(\"end\")\n",
    "                fact.to_parquet('data/us_equity/fundamentals/sec/'+ticker+'/'+key+'.parquet')\n",
    "            except:\n",
    "                print(key, 'fail - dei USD')\n",
    "\n",
    "            # Shares\n",
    "            try:\n",
    "                fact = pd.json_normalize(fact['units']['shares'])\n",
    "                fact[\"filed\"] = pd.to_datetime(fact[\"filed\"])\n",
    "                fact = fact.sort_values(\"end\")\n",
    "                fact.to_parquet('data/us_equity/fundamentals/sec/'+ticker+'/'+key+'.parquet')\n",
    "            except:\n",
    "                print(key, 'fail - dei shares')\n",
    "    except:\n",
    "        print(ticker + 'dei failed')\n",
    "\n",
    "    try:\n",
    "        for key in company_facts['facts']['us-gaap'].keys():\n",
    "            fact = company_facts['facts']['us-gaap'][key]\n",
    "\n",
    "            # USD\n",
    "            try:\n",
    "                fact = pd.json_normalize(fact['units']['USD'])\n",
    "                fact[\"filed\"] = pd.to_datetime(fact[\"filed\"])\n",
    "                fact = fact.sort_values(\"end\")\n",
    "                fact.to_parquet('data/us_equity/fundamentals/sec/'+ticker+'/'+key+'.parquet')\n",
    "            except:\n",
    "                print(key, ' fail - us-gaap')\n",
    "\n",
    "            # shares\n",
    "            try:\n",
    "                fact = pd.json_normalize(fact['units']['shares'])\n",
    "                fact[\"filed\"] = pd.to_datetime(fact[\"filed\"])\n",
    "                fact = fact.sort_values(\"end\")\n",
    "                fact.to_parquet('data/us_equity/fundamentals/sec/'+ticker+'/'+key+'.parquet')\n",
    "            except:\n",
    "                print(key, ' wrong units')\n",
    "\n",
    "            # USD/shares\n",
    "            try:\n",
    "                fact = pd.json_normalize(fact['units']['USD/shares'])\n",
    "                fact[\"filed\"] = pd.to_datetime(fact[\"filed\"])\n",
    "                fact = fact.sort_values(\"end\")\n",
    "                fact.to_parquet('data/us_equity/fundamentals/sec/'+ticker+'/'+key+'.parquet')\n",
    "            except:\n",
    "                print(key, ' wrong units')\n",
    "            # Store\n",
    "            try:\n",
    "                fact = pd.json_normalize(fact['units']['Store'])\n",
    "                fact[\"filed\"] = pd.to_datetime(fact[\"filed\"])\n",
    "                fact = fact.sort_values(\"end\")\n",
    "                fact.to_parquet('data/us_equity/fundamentals/sec/'+ticker+'/'+key+'.parquet')\n",
    "            except:\n",
    "                print(key, ' wrong units')\n",
    "            # pure\n",
    "            try:\n",
    "                fact = pd.json_normalize(fact['units']['pure'])\n",
    "                fact[\"filed\"] = pd.to_datetime(fact[\"filed\"])\n",
    "                fact = fact.sort_values(\"end\")\n",
    "                fact.to_parquet('data/us_equity/fundamentals/sec/'+ticker+'/'+key+'.parquet')\n",
    "            except:\n",
    "                print(key, ' wrong units')\n",
    "            # Year\n",
    "            try:\n",
    "                fact = pd.json_normalize(fact['units']['Year'])\n",
    "                fact[\"filed\"] = pd.to_datetime(fact[\"filed\"])\n",
    "                fact = fact.sort_values(\"end\")\n",
    "                fact.to_parquet('data/us_equity/fundamentals/sec/'+ticker+'/'+key+'.parquet')\n",
    "            except:\n",
    "                print(key, ' wrong units')\n",
    "    except:\n",
    "        print('second fail')\n",
    "    # print(tickers_cik.index[i]+'.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tickers = tickers_cik.index.values\n",
    "tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# os.mkdir('data/us_equity/fundamental/')\n",
    "# os.mkdir('data/us_equity/fundamental/sec/')\n",
    "for i in tqdm(range(len(tickers_cik))):\n",
    "    print(tickers_cik.index[i])\n",
    "    ticker = tickers_cik.index[i]\n",
    "    try:\n",
    "        os.mkdir('data/us_equity/fundamental/sec/{tick}/'.format(tick=ticker))\n",
    "    except:\n",
    "        print('mkdir failed for ticker: '+ticker)\n",
    "        pass\n",
    "\n",
    "    cik = tickers_cik['cik_str'].iloc[i]\n",
    "    # print(ticker, cik)\n",
    "    try:\n",
    "        company_facts = requests.get(\"https://data.sec.gov/api/xbrl/companyfacts/CIK\"+cik+'.json',headers=headers)\n",
    "        # print(company_facts)\n",
    "        company_facts = company_facts.json()\n",
    "    except:\n",
    "        print(ticker + ' error')\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        for key in company_facts['facts']['dei'].keys():\n",
    "            fact = company_facts['facts']['dei'][key]\n",
    "\n",
    "            # USD\n",
    "\n",
    "            try:\n",
    "                fact = pd.json_normalize(fact['units']['USD'])\n",
    "                fact[\"filed\"] = pd.to_datetime(fact[\"filed\"])\n",
    "                fact = fact.sort_values(\"end\")\n",
    "                fact.to_parquet('data/us_equity/fundamental/sec/{tick}/{key}.parquet'.format(tick=ticker,key=key))\n",
    "            except:\n",
    "                print(key, 'fail - dei USD')\n",
    "\n",
    "            # Shares\n",
    "            try:\n",
    "                fact = pd.json_normalize(fact['units']['shares'])\n",
    "                fact[\"filed\"] = pd.to_datetime(fact[\"filed\"])\n",
    "                fact = fact.sort_values(\"end\")\n",
    "                fact.to_parquet('data/us_equity/fundamental/sec/{tick}/{key}.parquet'.format(tick=ticker,key=key))\n",
    "            except:\n",
    "                print(key, 'fail - dei shares')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for key in tqdm(company_facts['facts']['us-gaap'].keys()):\n",
    "            fact = company_facts['facts']['us-gaap'][key]\n",
    "\n",
    "            # USD\n",
    "            try:\n",
    "                fact = pd.json_normalize(fact['units']['USD'])\n",
    "                fact[\"filed\"] = pd.to_datetime(fact[\"filed\"])\n",
    "                fact = fact.sort_values(\"end\")\n",
    "                fact.to_parquet('data/us_equity/fundamental/sec/{tick}/{key}.parquet'.format(tick=ticker,key=key))\n",
    "            except:\n",
    "                print(key, ' fail - us-gaap')\n",
    "\n",
    "            # shares\n",
    "            try:\n",
    "                fact = pd.json_normalize(fact['units']['shares'])\n",
    "                fact[\"filed\"] = pd.to_datetime(fact[\"filed\"])\n",
    "                fact = fact.sort_values(\"end\")\n",
    "                fact.to_parquet('data/us_equity/fundamental/sec/{tick}/{key}.parquet'.format(tick=ticker,key=key))\n",
    "            except:\n",
    "                print(key, ' wrong units')\n",
    "\n",
    "            # USD/shares\n",
    "            try:\n",
    "                fact = pd.json_normalize(fact['units']['USD/shares'])\n",
    "                fact[\"filed\"] = pd.to_datetime(fact[\"filed\"])\n",
    "                fact = fact.sort_values(\"end\")\n",
    "                fact.to_parquet('data/us_equity/fundamental/sec/{tick}/{key}.parquet'.format(tick=ticker,key=key))\n",
    "            except:\n",
    "                print(key, ' wrong units')\n",
    "            # Stock\n",
    "            try:\n",
    "                fact = pd.json_normalize(fact['units']['Stock'])\n",
    "                fact[\"filed\"] = pd.to_datetime(fact[\"filed\"])\n",
    "                fact = fact.sort_values(\"end\")\n",
    "                fact.to_parquet('data/us_equity/fundamental/sec/{tick}/{key}.parquet'.format(tick=ticker,key=key))\n",
    "            except:\n",
    "                print(key, ' wrong units')\n",
    "            # pure\n",
    "            try:\n",
    "                fact = pd.json_normalize(fact['units']['pure'])\n",
    "                fact[\"filed\"] = pd.to_datetime(fact[\"filed\"])\n",
    "                fact = fact.sort_values(\"end\")\n",
    "                fact.to_parquet('data/us_equity/fundamental/sec/{tick}/{key}.parquet'.format(tick=ticker,key=key))\n",
    "            except:\n",
    "                print(key, ' wrong units')\n",
    "            # Year\n",
    "            try:\n",
    "                fact = pd.json_normalize(fact['units']['Year'])\n",
    "                fact[\"filed\"] = pd.to_datetime(fact[\"filed\"])\n",
    "                fact = fact.sort_values(\"end\")\n",
    "                fact.to_parquet('data/us_equity/fundamental/sec/{tick}/{key}.parquet'.format(tick=ticker,key=key))\n",
    "            except:\n",
    "                print(key, ' wrong units')\n",
    "    except:\n",
    "        print(ticker + ' did not work')\n",
    "        pass\n",
    "\n",
    "    print(tickers_cik.index[i]+'.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_financials(stock_cik):\n",
    "    \"\"\"\n",
    "\n",
    "    :param stock_cik:\n",
    "    :type stock_cik:\n",
    "    :return:\n",
    "    :rtype:\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.get(f\"https://data.sec.gov/api/xbrl/companyconcept/{stock_cik}/us-gaap/Assets.json\".format(stock_cik=stock_cik), headers=headers)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response = requests.get(\"https://data.sec.gov/api/xbrl/companyconcept/CIK0000320193/us-gaap/Assets.json\", headers=headers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e913b7897ac30ba1cf0615ffc95e0dcac655edcf15ece0252fa093eec079069f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
