{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Using AlgoSeek for Algorithmic Trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import awswrangler as wr\n",
    "import pandas as pd \n",
    "import numpy as np  \n",
    "import seaborn as sns\n",
    "from tqdm import tqdm \n",
    "import os\n",
    "\n",
    "def get_trade_bucket_path(stock: str,year: int,month: int,day:int):\n",
    "    \"\"\"get the s3 path for trades for a stock\n",
    "    \n",
    "    args:\n",
    "        stock (str): \n",
    "        \n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    first_letter = stock[0]\n",
    "    bucket = \"s3://us-equity-1min-trades-\"+str(year)\n",
    "    path = bucket+'/{year}{month}{day}/{first_letter}/{stock}.csv.gz'.format(year,month,day,first_letter,stock)\n",
    "    return path\n",
    "\n",
    "pd.set_option('display.max_columns',1000)\n",
    "pd.set_option('display.max_rows',1000)\n",
    "\n",
    "s3 = boto3.resource('s3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Dataset Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be looking at the following AlgoSeek Datasets:\n",
    "\n",
    "- US Equity 1 Minute Trade and Quote (TAQ) \n",
    "- US Equity Options 1 Minute TAQ\n",
    "- US Equity Standard Daily Adjusted OHLC \n",
    "\n",
    "These are stored in S3 and are divided between several different buckets. They are stored in a standard format, allowing us to access data from specified days and equities. For each bucket in all of these datasets, there are index files that list metadata about the contents. We will first download all of these index files to use as reference when interacting with AlgoSeek's data.\n",
    "\n",
    "We will fetch index files for the last 6 years and store them for future use, then demonstrate two different methodologies for downloading and fetching data:\n",
    "\n",
    "- Downloading a month's data for all equities\n",
    "- Downloading the entire 6 years of data for a subset of equities\n",
    "\n",
    "In later notebooks, we will expand on these examples to show how to download data for stocks with the most volatility, liquidity, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Fetching the index files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "US Equity 1min TAQ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "US Equity Trade and Quote Minute Bar Buckets\n",
    "---------------------------------------------------------\n",
    "Bucket: s3://us-equity-1min-taq-yyyy where yyyy is a year (2016 - 2022)\n",
    "\n",
    "Path Format: us-equity-1min-taq-yyyy/yyyymmdd/s/sss.csv.gz\n",
    "\n",
    "Description: One csv.gz file per symbol per trading date where yyyymmdd is year, month and day, \n",
    "s - a single letter in A-Z range, sss - symbol\n",
    "\n",
    "Example: s3://us-equity-1min-taq-2022/20220104/I/IBM.csv.gz\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_equity_taq_1min = ['s3://us-equity-1min-taq-2017/.index/',\n",
    "                      's3://us-equity-1min-taq-2018/.index/',\n",
    "                      's3://us-equity-1min-taq-2019/.index/',\n",
    "                      's3://us-equity-1min-taq-2020/.index/',\n",
    "                      's3://us-equity-1min-taq-2021/.index/',\n",
    "                      's3://us-equity-1min-taq-2022/.index/'\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "US Equity Daily Standard Adjusted OHLC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"US Equity Standard Adjusted Daily OHLC Buckets\n",
    "---------------------------------------------------------\n",
    "Bucket: s3://us-equity-daily-ohlc-standard-adjusted-tradedate-yyyy where yyyy is a year \n",
    "(2016 - 2022)\n",
    "\n",
    "Path Format: us-equity-daily-ohlc-standard-adjusted-tradedate-yyyy/yyyymmdd.csv\n",
    "\n",
    "Description: One csv file per trading day where yyyymmdd is year, month and day\n",
    "\n",
    "Example: s3://us-equity-daily-ohlc-standard-adjusted-tradedate-2022/20220104.csv\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_equity_daily_ohlc = ['us-equity-daily-ohlc-standard-adjusted-tradedate-2017/.index/',\n",
    "                      'us-equity-daily-ohlc-standard-adjusted-tradedate-2018/.index/',\n",
    "                      'us-equity-daily-ohlc-standard-adjusted-tradedate-2019/.index/',\n",
    "                      'us-equity-daily-ohlc-standard-adjusted-tradedate-2020/.index/',\n",
    "                      'us-equity-daily-ohlc-standard-adjusted-tradedate-2021/.index/',\n",
    "                      'us-equity-daily-ohlc-standard-adjusted-tradedate-2022/.index/'\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "US Equity Options 1min TAQ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "US Options Trade and Quote Minute Bar Buckets\n",
    "---------------------------------------------------------\n",
    "Bucket: s3://us-options-1min-taq-yyyy where yyyy is a year (2016 - 2022)\n",
    "\n",
    "Path Format: us-options-1min-taq-yyyy/yyyymmdd/s/sss/sss.expdate.csv.gz\n",
    "\n",
    "Description: One csv.gz file per ticker, trading day and contract expiration date where yyyymmdd is year, month and day, s - a single letter in A-Z range, sss - symbol, expdate is the contract expiration date in yyyymmdd format\n",
    "\n",
    "Example: s3://us-options-1min-taq-2022/20220104/S/SPY/SPY.20220107.csv.gz\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_options_1min_taq = ['s3://us-options-1min-taq-2017/.index/',\n",
    "                      's3://us-options-1min-taq-2018/.index/',\n",
    "                      's3://us-options-1min-taq-2019/.index/',\n",
    "                      's3://us-options-1min-taq-2020/.index/',\n",
    "                      's3://us-options-1min-taq-2021/.index/',\n",
    "                      's3://us-options-1min-taq-2022/.index/'\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first much fetch the index files for each bucket we want to download from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = ['2017','2018','2019','2020','2021','2022']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2017 = []\n",
    "index2018 = []\n",
    "index2019 = []\n",
    "index2020 = []\n",
    "index2021 = []\n",
    "index2022 = []\n",
    "tradeindex = []\n",
    "\n",
    "indexfiles = [index2017,index2018,index2019,index2020,index2021,index2022]\n",
    "\n",
    "files2017 = []\n",
    "files2018 = []\n",
    "files2019 = []\n",
    "files2020 = []\n",
    "files2021 = []\n",
    "files2022 = []\n",
    "\n",
    "files = [files2017,files2018,files2019,files2020,files2021,files2022]\n",
    "\n",
    "counter=0\n",
    "\n",
    "for i in years:\n",
    "    print(i)\n",
    "    \n",
    "    bucket_name=\"us-equity-1min-trades-\"+i\n",
    "    \n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    \n",
    "    for obj in tqdm(bucket.objects.filter(Prefix=\".index/\",RequestPayer='requester')):\n",
    "        print(obj.key)\n",
    "        indexfiles[counter].append(obj.key)\n",
    "        files[counter].append('s3://'+bucket_name+'/'+obj.key)\n",
    "\n",
    "    counter = counter+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "counter = 0\n",
    "for i in tqdm(range(5)):\n",
    "    \n",
    "    bucket_name=\"us-equity-1min-trades-\"+years[i]\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    \n",
    "    for t in tqdm(range(len(indexfiles[counter]))):\n",
    "        local = 'trades/'+indexfiles[counter][t]\n",
    "        \n",
    "        with open(local, 'wb') as f:\n",
    "            bucket.download_fileobj(indexfiles[counter][t], f,ExtraArgs={'RequestPayer':'requester'})\n",
    "        \n",
    "        # s3.Bucket(bucket_name18).download_file(files2018[253:][i],local,ExtraArgs={'RequestPayer':'requester'})\n",
    "        \n",
    "    counter = counter+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Download Daily OHLC Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the daily dataset is significantly smaller and is used to filter/select stocks for fetching 1 minute Trade and Quote data, we will just go ahead and download the whole thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in us_equity_daily_ohlc:\n",
    "    print(i)\n",
    "    \n",
    "    bucket_name=i[5:-7]\n",
    "    \n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    \n",
    "    for obj in tqdm(bucket.objects.filter(Prefix=\".index/\",RequestPayer='requester')):\n",
    "        print(obj.key)\n",
    "        \n",
    "        local='daily/'+obj.key\n",
    "        path, filename = os.path.split(obj.key)\n",
    "\n",
    "        with open(local, 'wb') as f:\n",
    "            bucket.download_fileobj(bucket_name+filename, f,ExtraArgs={'RequestPayer':'requester'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Downloading 1 Minute TAQ Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple different ways to download the"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
